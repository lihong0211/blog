
### 一、什么是 Word2Vec？

**Word2Vec** 并不是一个单一的算法，而是一个在 2013 由 Google 的 Tomas Mikolov 等人提出的一系列**模型和工具**。它的核心思想是：**将一个词语映射到一个高维的向量空间中，并且让语义上相似的词语在这个空间中的向量位置也彼此接近。**

简单来说，Word2Vec 的核心理念是：
**“一个单词的含义，可以由它周围经常出现的单词（上下文）来定义。”** 这被称为 **分布假说**。

* **输入**：大量的文本数据。
* **输出**：一个“词向量模型”。在这个模型中，每个单词都被表示成一个固定长度的向量（比如300维）。
* **目标**：语义相似的词，其向量的**余弦相似度**很高（即方向接近）；同时，词向量还能捕捉到复杂的语义关系。

---

### 二、核心模型：两种训练方式

Word2Vec 提供了两种主要的模型架构来实现上述思想，它们都使用简单的神经网络，但训练任务不同。

#### 1. CBOW（连续词袋模型）

* **目标**：**通过上下文来预测中心词**。
* **工作原理**：模型将目标词语周围的所有上下文词语（例如，前后各2个词）的向量求和或平均，然后尝试预测中间的那个目标词语。
* **比喻**：给你一段话中挖掉的一个空，让你根据空周围的词来猜这个空应该填什么词。
* **特点**：训练速度快，对高频词更准确。

#### 2. Skip-gram（跳字模型）

* **目标**：**通过中心词来预测其上下文**。
* **工作原理**：给定一个中心词语，模型尝试预测它周围一定窗口大小内的所有上下文词语。
* **比喻**：给你一个关键词，让你写出它周围可能出现的词。
* **特点**：在小型数据集上表现更好，尤其能很好地处理低频词。

**简单对比：**

- **CBOW**: `上下文 -> 中心词`
- **Skip-gram**: `中心词 -> 上下文`

在实践中，**Skip-gram 模型通常能产生质量更高的词向量**，尤其是在捕捉复杂的语义关系方面，因此更为常用。

---

### 三、一个详细的 Skip-gram 例子

假设我们有一个非常简单的句子，并且设定窗口大小为2：

> “The quick brown fox jumps.”

**1. 创建训练样本：**
我们以中心词 `brown` 为例，它的上下文（窗口为2）是 `[The, quick, fox, jumps]`。Skip-gram 会生成以下（输入，输出）训练对：

- (brown, The)
- (brown, quick)
- (brown, fox)
- (brown, jumps)

对整个语料库中的每一个词都进行这样的操作，我们会得到数百万甚至数十亿个这样的训练对。

**2. 模型结构（简化）：**

- **输入层**：一个大小为 `V` 的 one-hot 向量，其中 `V` 是词汇表的大小。例如，如果我们的词汇表是 `[The, quick, brown, fox, jumps]`，那么 `brown` 的 one-hot 向量就是 `[0, 0, 1, 0, 0]`。
- **隐藏层**：没有激活函数，只是一个权重矩阵 `W`（大小为 `V x N`，其中 `N` 是我们想要的词向量维度，比如300）。**这个权重矩阵 `W` 就是我们最终要学习的词向量表！** 当 one-hot 向量输入时，实际上只是选中了 `W` 矩阵中的某一行。所以，`brown` 的词向量就是 `W` 矩阵的第三行。
- **输出层**：一个大小为 `V` 的向量，使用 Softmax 函数将其转换为概率分布，表示每个词作为上下文词出现的概率。

**3. 训练过程：**

- 我们输入 `brown` 的 one-hot 向量 `[0,0,1,0,0]`。
- 隐藏层计算：`隐藏向量 = 输入向量 · W`。由于输入是 one-hot，结果直接就是 `W` 矩阵中对应 `brown` 的那一行。我们称这个向量为 `v_brown`。
- 输出层计算：`输出向量 = 隐藏向量 · W‘`（这里 `W‘` 是另一个权重矩阵，可以理解为“上下文词向量矩阵”）。然后通过 Softmax 得到概率。
- 我们希望输出概率中，`The`, `quick`, `fox`, `jumps` 对应的位置的概率尽可能高。
- 通过反向传播和梯度下降（通常使用负采样技术来加速训练），不断调整权重矩阵 `W` 和 `W‘`。

**4. 最终结果：**
训练完成后，我们丢弃输出层，只保留**隐藏层的权重矩阵 `W`**。这个矩阵的每一行，就对应词汇表中一个词的 **词向量**。

---

### 四、Word2Vec 的神奇之处：语义和语法关系

词向量的强大之处在于，向量之间的空间关系反映了词语之间的语义和语法关系。

**1. 相似性：**
词语 `king`、`queen`、`man`、`woman` 的向量在空间中会聚集在不同的“簇”中，但 `king` 和 `queen` 的距离，与 `man` 和 `woman` 的距离会非常相似。

**2. 类比关系（最著名的例子）：**

**“国王 - 男人 + 女人 ≈ 女王”**

用向量运算表示就是：
`vector(‘king’) - vector(‘man’) + vector(‘woman’) ≈ vector(‘queen’)`

这意味着词向量空间捕捉到了“性别”这一关系。

其他例子：

- `vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) ≈ vector(‘Rome’)` （首都关系）
- `vector(‘walked’) - vector(‘walking’) + vector(‘swimming’) ≈ vector(‘swam’)` （时态关系）

---

### 五、关键技术与优化

原始的 Word2Vec 训练效率很低，因为词汇表 `V` 可能非常大（数百万），导致输出层 Softmax 计算量巨大。因此引入了两种关键技术：

1. **负采样**

   * **思想**：我们不再要求模型一次预测所有上下文词的概率。对于每个真实的（中心词，上下文词）对（正样本），我们随机从词汇表中抽取 `k` 个“非上下文”词（负样本）。
   * **新任务**：训练模型将一个**二分类任务**：区分一个词对（中心词，目标词）是真实的上下文关系（正类）还是随机组合的关系（负类）。
   * **效果**：这极大地简化了计算，成为训练 Word2Vec 的标准方法。论文中推荐小数据集 `k=5-20`，大数据集 `k=2-5`。
2. **层次 Softmax**

   * **思想**：使用一棵哈夫曼树来表示整个词汇表，其中每个叶子节点代表一个单词。计算一个词的概率不再需要遍历整个词汇表，而只需要沿着树路径计算，复杂度从 `O(V)` 降为 `O(log(V))`。
   * **效果**：同样是为了加速训练，尤其适用于低频词。

---

### 六、总结

| 特性               | 描述                                                                                          |
| :----------------- | :-------------------------------------------------------------------------------------------- |
| **核心思想** | 基于分布假说，用上下文定义词义。                                                              |
| **输出**     | 高维空间中的词向量（嵌入）。                                                                  |
| **主要模型** | **CBOW**（上下文预测中心词），**Skip-gram**（中心词预测上下文）。                 |
| **关键创新** | 简单的神经网络结构，配合**负采样**或**层次Softmax**进行高效训练。                 |
| **核心能力** | 捕捉词语的**语义相似性**和复杂的**类比关系**（如 king - man + woman = queen）。   |
| **应用**     | 作为几乎所有NLP任务的**基础特征**：文本分类、情感分析、机器翻译、智能问答、推荐系统等。 |

Word2Vec 的意义在于，它首次用一种高效且可扩展的方式，将人类的语言词汇“数字化”为计算机能够理解和计算的数学对象（向量），并且这些向量中蕴含了丰富的语义信息，为后续的深度学习NLP研究奠定了坚实的基础。
