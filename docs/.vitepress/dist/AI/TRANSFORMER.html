<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>深入理解 Transformer：为什么它改变了 AI 的一切 | 技术文档</title>
    <meta name="description" content="技术学习文档集合">
    <link rel="stylesheet" href="/docs/assets/style.ee6dddf9.css">
    <link rel="modulepreload" href="/docs/assets/app.2a30658a.js">
    <link rel="modulepreload" href="/docs/assets/AI_TRANSFORMER.md.e49385c8.lean.js">
    
    <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-9c83c81e><!--[--><!--]--><!--[--><span tabindex="-1" data-v-be9c27de></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-be9c27de> Skip to content </a><!--]--><!----><header class="VPNav" data-v-9c83c81e data-v-bd98ce1b><div class="VPNavBar has-sidebar" data-v-bd98ce1b data-v-be850364><div class="container" data-v-be850364><div class="VPNavBarTitle has-sidebar" data-v-be850364 data-v-ef6bdfee><a class="title" href="/docs/" data-v-ef6bdfee><!--[--><!--]--><!----><!--[-->技术文档<!--]--><!--[--><!--]--></a></div><div class="content" data-v-be850364><!--[--><!--]--><!----><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-be850364 data-v-4f0f7d90><span id="main-nav-aria-label" class="visually-hidden" data-v-4f0f7d90>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/" data-v-4f0f7d90 data-v-d1df1d79 data-v-cbb71e82><!--[-->首页<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/docs/AI/AIGC.html" data-v-4f0f7d90 data-v-d1df1d79 data-v-cbb71e82><!--[-->文档<!--]--><!----></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-be850364 data-v-6975dfb0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-6975dfb0 data-v-a11464a8 data-v-9b61e15c><span class="check" data-v-9b61e15c><span class="icon" data-v-9b61e15c><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-a11464a8><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-a11464a8><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-be850364 data-v-9d62d057 data-v-2aee37ba><!--[--><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-be850364 data-v-af83da42 data-v-0bf5642f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-0bf5642f><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-0bf5642f><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-0bf5642f><div class="VPMenu" data-v-0bf5642f data-v-958884f0><!----><!--[--><!--[--><!----><div class="group" data-v-af83da42><div class="item appearance" data-v-af83da42><p class="label" data-v-af83da42>Appearance</p><div class="appearance-action" data-v-af83da42><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-af83da42 data-v-a11464a8 data-v-9b61e15c><span class="check" data-v-9b61e15c><span class="icon" data-v-9b61e15c><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-a11464a8><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-a11464a8><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-af83da42><div class="item social-links" data-v-af83da42><div class="VPSocialLinks social-links-list" data-v-af83da42 data-v-2aee37ba><!--[--><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-be850364 data-v-cd76eed2><span class="container" data-v-cd76eed2><span class="top" data-v-cd76eed2></span><span class="middle" data-v-cd76eed2></span><span class="bottom" data-v-cd76eed2></span></span></button></div></div></div><!----></header><div class="VPLocalNav" data-v-9c83c81e data-v-05a75127><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-05a75127><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-05a75127><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-05a75127>Menu</span></button><a class="top-link" href="#" data-v-05a75127> Return to top </a></div><aside class="VPSidebar" data-v-9c83c81e data-v-66069f15><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-66069f15><span class="visually-hidden" id="sidebar-aria-label" data-v-66069f15> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-66069f15><section class="VPSidebarGroup" data-v-66069f15 data-v-7785691c><div class="title" data-v-7785691c><h2 class="title-text" data-v-7785691c>AI</h2><div class="action" data-v-7785691c><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 24 24" class="icon minus" data-v-7785691c><path d="M19,2H5C3.3,2,2,3.3,2,5v14c0,1.7,1.3,3,3,3h14c1.7,0,3-1.3,3-3V5C22,3.3,20.7,2,19,2zM20,19c0,0.6-0.4,1-1,1H5c-0.6,0-1-0.4-1-1V5c0-0.6,0.4-1,1-1h14c0.6,0,1,0.4,1,1V19z"></path><path d="M16,11H8c-0.6,0-1,0.4-1,1s0.4,1,1,1h8c0.6,0,1-0.4,1-1S16.6,11,16,11z"></path></svg><svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" class="icon plus" data-v-7785691c><path d="M19,2H5C3.3,2,2,3.3,2,5v14c0,1.7,1.3,3,3,3h14c1.7,0,3-1.3,3-3V5C22,3.3,20.7,2,19,2z M20,19c0,0.6-0.4,1-1,1H5c-0.6,0-1-0.4-1-1V5c0-0.6,0.4-1,1-1h14c0.6,0,1,0.4,1,1V19z"></path><path d="M16,11h-3V8c0-0.6-0.4-1-1-1s-1,0.4-1,1v3H8c-0.6,0-1,0.4-1,1s0.4,1,1,1h3v3c0,0.6,0.4,1,1,1s1-0.4,1-1v-3h3c0.6,0,1-0.4,1-1S16.6,11,16,11z"></path></svg></div></div><div class="items" data-v-7785691c><!--[--><!--[--><a class="VPLink link link" href="/docs/AI/AIGC.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>AIGC</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/COZE_DIFY.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>COZE & DIFY</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/DashScope_ModelScope.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>DashScope & ModelScope</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/DDG_SERPAPI.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>DDG & SERPAPI</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/FAISS.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>FAISS</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/GPT%E6%9E%B6%E6%9E%84.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>GPT架构</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/LANGCHAIN.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>LANGCHAIN</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/LCEL.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>LCEL</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/MODELS.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>MODELS</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/MOE.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>MOE</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/NLP.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>NLP</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/QIANWEN_VL.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>QIANWEN VL</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/SERPAPI.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>SERPAPI</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/WORD2VEC.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>WORD2VEC</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E6%A8%A1%E5%BC%8F.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>大模型应用开发模式</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>智能体系统设计</span><!--]--><!----></a><!----><!--]--><!--[--><a class="VPLink link link" href="/docs/AI/%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F-%E7%A8%80%E7%96%8F%E5%90%91%E9%87%8F.html" style="padding-left:0px;" data-v-5686a8aa data-v-cbb71e82><!--[--><span class="link-text" data-v-5686a8aa>稠密向量-稀疏向量</span><!--]--><!----></a><!----><!--]--><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-9c83c81e data-v-954ad629><div class="VPDoc has-sidebar has-aside" data-v-954ad629 data-v-dc19973f><div class="container" data-v-dc19973f><div class="aside" data-v-dc19973f><div class="aside-curtain" data-v-dc19973f></div><div class="aside-container" data-v-dc19973f><div class="aside-content" data-v-dc19973f><div class="VPDocAside" data-v-dc19973f data-v-63938968><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-63938968 data-v-72b1abc9><div class="content" data-v-72b1abc9><div class="outline-marker" data-v-72b1abc9></div><div class="outline-title" data-v-72b1abc9>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-72b1abc9><span class="visually-hidden" id="doc-outline-aria-label" data-v-72b1abc9> Table of Contents for current page </span><ul class="root" data-v-72b1abc9 data-v-a950ca15><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-63938968></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-dc19973f><div class="content-container" data-v-dc19973f><!--[--><!--]--><main class="main" data-v-dc19973f><div style="position:relative;" class="vp-doc _docs_AI_TRANSFORMER" data-v-dc19973f><div><h1 id="深入理解-transformer-为什么它改变了-ai-的一切" tabindex="-1">深入理解 Transformer：为什么它改变了 AI 的一切 <a class="header-anchor" href="#深入理解-transformer-为什么它改变了-ai-的一切" aria-hidden="true">#</a></h1><p>最近在学习大语言模型的时候，总是绕不开 <strong>Transformer</strong> 这个架构。无论是 GPT、BERT 还是 Gemini，它们的核心都是 Transformer。今天想写篇文章，把我对 Transformer 的理解整理一下，也希望能帮助到正在学习的朋友们。</p><p>我会从为什么需要 Transformer、它的核心思想、架构细节以及它的影响这几个方面来聊聊。</p><hr><h2 id="为什么需要-transformer" tabindex="-1">为什么需要 Transformer？ <a class="header-anchor" href="#为什么需要-transformer" aria-hidden="true">#</a></h2><p>在 Transformer 出现之前（2017 年以前），处理序列任务（比如翻译、文本生成）主要用的是<strong>循环神经网络（RNN）</strong>，以及它的改进版本 LSTM 和 GRU。</p><p>但 RNN 有两个让我很头疼的问题：</p><ol><li><strong>必须顺序处理</strong>：它得一个词一个词地按顺序计算，完全没法并行，训练速度慢得让人抓狂。</li><li><strong>长程依赖问题</strong>：当文本很长的时候，开头的信息很难传递到末尾，模型就像会&quot;失忆&quot;一样。</li></ol><p>所以当 Google 在 2017 年发表那篇著名的论文《Attention Is All You Need》时，我第一反应是：终于有人要解决这些问题了！</p><p>Transformer 完全抛弃了循环结构，**只用&quot;注意力机制&quot;**来建模序列中所有元素之间的关系。这样做带来了两个巨大的优势：</p><ol><li><strong>极高的并行化能力</strong>，训练时间大幅缩短</li><li><strong>强大的长程依赖建模能力</strong>，无论两个词距离多远，都能直接建立联系</li></ol><hr><h2 id="transformer-的核心-自注意力机制" tabindex="-1">Transformer 的核心：自注意力机制 <a class="header-anchor" href="#transformer-的核心-自注意力机制" aria-hidden="true">#</a></h2><p>这是 Transformer 的灵魂所在。理解它，需要掌握三个核心概念：<strong>Query、Key、Value（Q、K、V）</strong>。</p><p>让我用个例子来解释：</p><p>想象你在读一篇论文，想找它的创新点。这时候：</p><ul><li><strong>Query</strong> 就是你脑子里的问题：&quot;这篇论文的创新点是什么？&quot;</li><li><strong>Key</strong> 就是论文中每个句子的主题摘要</li><li><strong>Value</strong> 就是每个句子的实际内容</li></ul><p>你会把问题（Query）和每个句子的主题（Key）进行比对，看哪个最相关。然后根据相关程度，从对应的句子（Value）中提取信息来回答你的问题。</p><p>在 Transformer 中，这个过程是这样的（以&quot;我爱AI&quot;这句话为例）：</p><ol><li>先把每个词转换成向量</li><li>为每个词生成三个向量：<strong>Q（我想问的）</strong>、<strong>K（我能回答的）</strong>、<strong>V（我实际包含的信息）</strong></li><li>用当前词的 Q 去和所有词的 K 做点乘，得到注意力分数。分数越高，说明关系越密切</li><li>把分数做 Softmax 归一化，得到权重（所有权重加起来等于 1）</li><li>用这些权重对所有词的 V 做加权求和，得到当前词的注意力输出</li></ol><p>这样，每个词在计算输出时，都能直接&quot;看到&quot;并融合整个序列的信息。</p><p><strong>多头注意力</strong>的设计也很巧妙。为了捕捉不同类型的关系（比如语法关系、语义关系、指代关系等），Transformer 会并行运行多个独立的&quot;自注意力&quot;层（多个&quot;头&quot;），然后把结果拼接起来。这样模型就能在不同子空间里学习不同的关联模式。</p><hr><h2 id="transformer-架构详解" tabindex="-1">Transformer 架构详解 <a class="header-anchor" href="#transformer-架构详解" aria-hidden="true">#</a></h2><p>Transformer 采用的是**编码器-解码器（Encoder-Decoder）**结构，最初是为机器翻译设计的。不过编码器和解码器也可以单独使用，比如 BERT 只用编码器，GPT 只用解码器。</p><h3 id="编码器部分" tabindex="-1">编码器部分 <a class="header-anchor" href="#编码器部分" aria-hidden="true">#</a></h3><p>编码器由 N 个（原论文中 N=6）完全相同的层堆叠而成。每一层包含两个子层：</p><ol><li><strong>多头自注意力层</strong>：让序列中的每个词相互关注，提取上下文信息</li><li><strong>前馈神经网络层</strong>：一个全连接网络，对每个位置的输出做独立的非线性变换</li></ol><p>每个子层周围还有两个关键技巧：</p><ul><li><strong>残差连接</strong>：把子层的输入直接加到输出上（<code>输出 = 子层(输入) + 输入</code>）。这能缓解深度网络中的梯度消失问题，让模型可以堆得很深</li><li><strong>层归一化</strong>：在残差连接之后做层归一化，稳定训练过程</li></ul><h3 id="解码器部分" tabindex="-1">解码器部分 <a class="header-anchor" href="#解码器部分" aria-hidden="true">#</a></h3><p>解码器也由 N 个相同的层堆叠而成。每一层包含三个子层：</p><ol><li><strong>带掩码的多头自注意力层</strong>：这是关键！在训练时，为了确保对位置 i 的预测只依赖于位置小于 i 的输出（防止&quot;偷看&quot;未来信息），会用&quot;掩码&quot;把当前位置之后的所有连接屏蔽掉。这给了 Transformer <strong>自回归生成</strong>的能力</li><li><strong>编码器-解码器注意力层</strong>：这里的 Query 来自解码器上一层的输出，而 Key 和 Value 来自<strong>编码器最终的输出</strong>。这样解码器在生成每个词时，都能有选择地聚焦于输入序列的不同部分</li><li><strong>前馈神经网络层</strong>：和编码器一样</li></ol><h3 id="其他关键组件" tabindex="-1">其他关键组件 <a class="header-anchor" href="#其他关键组件" aria-hidden="true">#</a></h3><ul><li><strong>位置编码</strong>：因为 Transformer 没有循环和卷积，它本身感知不到词的顺序。所以需要手动加入<strong>位置编码</strong>——一组和词向量维度相同的、表示位置信息的向量，和词向量相加后输入模型。这些编码通过正弦/余弦函数生成，能让模型理解&quot;相对位置&quot;</li><li><strong>嵌入层</strong>：把输入的词标记（Token）转换成向量</li><li><strong>最终输出层</strong>：解码器输出后，通过一个线性层和 Softmax 层，把向量转换成所有目标词汇的概率分布，从而预测下一个词</li></ul><hr><h2 id="transformer-的影响与变体" tabindex="-1">Transformer 的影响与变体 <a class="header-anchor" href="#transformer-的影响与变体" aria-hidden="true">#</a></h2><p>Transformer 的影响真的是划时代的：</p><ol><li><strong>催生了大语言模型时代</strong>：GPT、BERT、T5、PaLM、LLaMA 等所有现代 LLM 都基于 Transformer 架构</li><li><strong>统一了多模态 AI</strong>：Transformer 不局限于文本。通过把图像、音频、视频等数据切分成&quot;块&quot;并视为序列，催生了 Vision Transformer、多模态模型（如 CLIP、DALL-E）</li><li><strong>成为 AI 基础模型</strong>：其&quot;注意力&quot;核心思想已成为 AI 领域最强大的模式提取工具之一</li></ol><p>主要的变体家族包括：</p><ul><li><strong>纯编码器模型</strong>：擅长理解任务。比如 <strong>BERT</strong>（双向编码表示），通过完形填空的方式预训练，在文本分类、问答等任务上表现很好</li><li><strong>纯解码器模型</strong>：擅长生成任务。比如 <strong>GPT 系列</strong>（生成式预训练 Transformer），通过自回归的下一个词预测进行训练，擅长文本生成、对话、代码生成等</li><li><strong>编码器-解码器模型</strong>：擅长序列到序列任务。比如 <strong>T5</strong>、<strong>BART</strong>，把所有任务都格式化成&quot;文本到文本&quot;的转换，适合翻译、摘要等</li></ul><hr><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-hidden="true">#</a></h2><p>Transformer 是一个基于<strong>自注意力机制</strong>的深度神经网络架构。它通过 <strong>Query-Key-Value</strong> 机制让序列中的每个元素都能直接与所有其他元素互动，完美解决了长程依赖问题，并实现了空前的并行化。</p><p>其<strong>编码器-解码器</strong>结构以及<strong>位置编码</strong>、<strong>残差连接</strong>等精巧设计，让它成为过去几年 AI 领域最具统治力的架构，直接推动了生成式 AI 革命的到来。</p><p>简单来说，Transformer 的核心就是：<strong>让模型自己学会，在处理信息时应该&quot;注意&quot;哪里。</strong> 这种灵活而强大的能力，让它成为了当今 AI 的基石。</p></div></div></main><!--[--><!--]--><footer class="VPDocFooter" data-v-dc19973f data-v-5440e431><!----><div class="prev-next" data-v-5440e431><div class="pager" data-v-5440e431><!----></div><div class="pager" data-v-5440e431><a class="pager-link next" href="/docs/AI/AIGC.html" data-v-5440e431><span class="desc" data-v-5440e431>Next page</span><span class="title" data-v-5440e431>AIGC</span></a></div></div></footer><!--[--><!--]--></div></div></div></div></div><footer class="VPFooter has-sidebar" data-v-9c83c81e data-v-0bba5d65><div class="container" data-v-0bba5d65><p class="message" data-v-0bba5d65>技术文档集合</p><p class="copyright" data-v-0bba5d65>Copyright © 2024</p></div></footer><!--[--><!--]--></div></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"ai_aigc.md\":\"7e96d0af\",\"ai_coze_dify.md\":\"8e8b8ed4\",\"ai_ddg_serpapi.md\":\"2faa92fe\",\"ai_dashscope_modelscope.md\":\"536ed077\",\"ai_faiss.md\":\"b1d3e12a\",\"ai_gpt架构.md\":\"c1ee6976\",\"ai_langchain.md\":\"722db8a6\",\"ai_lcel.md\":\"2a7516e9\",\"ai_models.md\":\"d396cbb7\",\"ai_moe.md\":\"8f51fc73\",\"ai_nlp.md\":\"466e755d\",\"ai_qianwen_vl.md\":\"55d4d78c\",\"ai_rag.md\":\"37f05a5e\",\"ai_serpapi.md\":\"88f17047\",\"ai_transformer.md\":\"e49385c8\",\"ai_word2vec.md\":\"2ebd9b15\",\"ai_大模型应用开发模式.md\":\"545c1cd6\",\"ai_智能体系统设计.md\":\"fae1cbb6\",\"ai_稠密向量-稀疏向量.md\":\"493c7f5a\",\"backend_acid.md\":\"5ceb59fb\",\"backend_acid_my.md\":\"96a2ed5b\",\"backend_python gil 影响与解决方案.md\":\"b1d03e54\",\"backend_restfulapi.md\":\"3d93312e\",\"backend_database_basis.md\":\"ff20ecf4\",\"backend_database_character-set.md\":\"3a3a8b59\",\"backend_database_nosql.md\":\"6ea14bb8\",\"backend_database_redis_redis-questions-01.md\":\"300014a6\",\"backend_database_redis_redis-questions-02.md\":\"9171b6b8\",\"backend_database_redis_数据结构.md\":\"a8ac6131\",\"backend_database_sql_sql-syntax-summary.md\":\"a30a555d\",\"backend_docker_docker-in-action.md\":\"58a063c6\",\"backend_docker_docker-intro.md\":\"f01420c3\",\"backend_index.md\":\"1aaf6918\",\"backend_分库分表策略.md\":\"5c7629e1\",\"backend_协程.md\":\"255322a8\",\"backend_死锁.md\":\"779b1f6d\",\"backend_进程-线程-协程.md\":\"819bf76b\",\"index.md\":\"b4771d2a\"}")</script>
    <script type="module" async src="/docs/assets/app.2a30658a.js"></script>
    
  </body>
</html>